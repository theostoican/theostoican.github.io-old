<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>SCAFFOLD - Stochastic Controlled Averaging for Federated Learning | Theodor Stoican</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="SCAFFOLD - Stochastic Controlled Averaging for Federated Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Jekyll also offers powerful support for code snippets:" />
<meta property="og:description" content="Jekyll also offers powerful support for code snippets:" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2020/11/28/scaffold.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2020/11/28/scaffold.html" />
<meta property="og:site_name" content="Theodor Stoican" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-28T15:43:57+01:00" />
<script type="application/ld+json">
{"description":"Jekyll also offers powerful support for code snippets:","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2020/11/28/scaffold.html"},"url":"http://localhost:4000/jekyll/update/2020/11/28/scaffold.html","headline":"SCAFFOLD - Stochastic Controlled Averaging for Federated Learning","dateModified":"2020-11-28T15:43:57+01:00","datePublished":"2020-11-28T15:43:57+01:00","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Theodor Stoican" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Theodor Stoican</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">SCAFFOLD - Stochastic Controlled Averaging for Federated Learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-11-28T15:43:57+01:00" itemprop="datePublished">Nov 28, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>

<ul>
  <li>Federated Learning detour</li>
  <li>Statistical heterogeneity</li>
  <li>notation</li>
  <li>some gifs with animations</li>
  <li>fedprox intro</li>
</ul>

<p>This is meant to be an attempt to make a paper review so that people reading this can compare their understanding with this.</p>
<h1 id="detour---federated-learning">Detour - Federated Learning</h1>
<p>Many fields have hard requirements in terms of how data is distributed and handled. In the medical domain, for instance, where data is extremely sensitive, data privacy is particularly important and, for this reason, medical institutions should be reluctant to share it by any means.</p>

<p>At the same time, the medical domain could greatly benefit from the newest approaches in Machine Learning in order to improve the diagnostic capabilities of doctors (detection of malignant cells in IMR, macular degeneration, and so forth). Thus, data privacy and applied machine learning in the medical domain creates a dichotomy that brings the progress in this area at a stalemate.</p>

<p><code class="language-plaintext highlighter-rouge">Federated Learning</code> is meant to be a solution to this problem. In a nutshell, it changes the paradigm of training a machine learning model by bringing the model to the data, instead of bringing the data to the model.</p>

<p>More specifically, assume that there is a server (e.g. a machine learning institution) which contains the model that we wish to train and a couple of nodes (e.g. hospitals) that contain the data required for training. Instead of gathering all the data to the server and train locally, we proceed as follows:</p>
<ul>
  <li>send the model from the server to the nodes</li>
  <li>compute a couple of training iterations on each of the nodes</li>
  <li>send back the updated models from each of the nodes back to the server</li>
  <li>aggregate this models and obtain a new, better model</li>
  <li>repeat</li>
</ul>
<figure>
    <img src="/img/fedlearning.gif" alt="missing" />
    <figcaption align="center">[1] © https://medium.com/secure-and-private-ai-writing-challenge/federated-learning-an-introduction-93bc0167f916</figcaption>
</figure>
<p>The only thing missing in the steps from above is the type of aggregation that we would like to make in order to obtain the new model.</p>

<h1 id="fedavg">FedAvg</h1>
<p>One of the simplest and most commonly used algorithm in Federated Learning is Federated Averaging (FedAvg). The inner working of FedAvg is pretty straightforward. Let us have a look at [2].</p>

<figure>
    <img src="/img/fedavg.png" alt="missing" />
    <figcaption align="center">[2] © https://proandroiddev.com/federated-learning-e79e054c33ef</figcaption>
</figure>

<p>In a nutshell, the central server sends to the nodes the model. Each of the nodes may compute a different number of iterations based on their own computational resources (illustrated here via the usage of mobile phones or laptops). Then, the nodes send back the <img src="https://render.githubusercontent.com/render/math?math=\Delta w^i" /> (the difference between the new weights computed locally and the weights initially received on node i). The model then aggregates these new weights based on the formula from below, which essentially quantifies how much contribution can each of the clients can bring, based on the number of local updates/iterations.</p>

<figure>
    <img src="/img/fedavgformula.png" alt="missing" />
    <figcaption align="center">[3] © https://www.inovex.de/blog/federated-learning-collaborative-training-part-1/</figcaption>
</figure>

<h1 id="federated-learning---known-issues">Federated Learning - Known Issues</h1>

<p>There are 4 well-known issues with federated learning:</p>
<figure>
    <img src="/img/fedlearningissues.gif" alt="missing" />
    <figcaption align="center">[3] © https://www.inovex.de/blog/federated-learning-collaborative-training-part-1/</figcaption>
</figure>

<p>We will handle the issue of <code class="language-plaintext highlighter-rouge">Statistical Heterogeneity</code> in this post. What this means is that the distribution of data can be different on the many nodes, leading to different local optima and, hence, to the convergence to different solutions. This essentially means that:</p>
<blockquote>
  <p>Non-IID data on different clients could lead to different local optima, depending on the local distribution of the data.</p>
</blockquote>

<p>General things to do: paper annotations, gifs, quotes like Karpathy, code, put in a chat, dos and dont’s for a blog post</p>
<h2 id="fedavg---issues">FedAvg - Issues</h2>
<p>The paper, which is the main focus of this post, explores one of the limitations of <code class="language-plaintext highlighter-rouge">FedAvg</code> in the context of the aforementioned <code class="language-plaintext highlighter-rouge">Statistical Heterogeneity</code>. Before getting started, let us have a look at the notation used throughout this post as well as throughout the paper:</p>

<figure>
    <img src="/img/notation.png" alt="missing" />
</figure>

<p>It may look like hieroglyphs for now, but don’t worry. They should make sense once we reach the theoretical parts of the paper. Just keep in mind that, in case something is unclear, go back here at the notation and see what that represents.</p>

<h1 id="intuition">Intuition</h1>
<p>The driving mechanism under any machine learning training problem is a loss function. As always, even within Federated Learning, we’re trying to minimize a loss function. The loss function looks like slightly different though.</p>

<p>To begin with, let us have a look at the local loss function (on a particular node i):</p>
<figure>
    <img src="/img/framework2.png" alt="missing" />
</figure>

<p>We know that locally we have access to a batch of the data when we make a local update. That explains the expression under the <em>expected value</em> from above. Its value is dependent both on the data ‘’’ latex f’’’</p>

<figure>
    <img src="/img/framework1.png" alt="missing" />
</figure>
<p>\(sasdaada\)</p>

<h1 id="math-detour">Math Detour</h1>

<ul>
  <li>the paper which will be the recurring theme of this blog post explores the fundamental issue with fedavg in the context presented above - statistical heterogeneity</li>
  <li>notation
    <h2 id="intuition-of-the-problems">intuition of the problems</h2>
    <h2 id="math-detour-1">Math detour</h2>
    <h2 id="theoretical-issues">Theoretical issues</h2>
    <h2 id="novel-result-newly---new-bound-on-fedavg">Novel result Newly - new bound on FedAVg</h2>
    <h1 id="scaffold">SCAFFOLD</h1>
    <h2 id="intuition-1">Intuition</h2>
    <h2 id="theoretical-superiority">Theoretical superiority</h2>
    <h2 id="practical-superiority">Practical superiority</h2>
    <h3 id="results">results</h3>
    <h2 id="in-a-nutshell">In a nutshell..</h2>
    <h2 id="vision-for-the-future">Vision for the future</h2>
    <h2 id="critique-on-my-side">Critique on my side</h2>
    <h3 id="fedprox---explanation">fedprox - explanation</h3>
  </li>
</ul>

  </div><a class="u-url" href="/jekyll/update/2020/11/28/scaffold.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Theodor Stoican</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Theodor Stoican</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/theostoican"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">theostoican</span></a></li><li><a href="https://www.twitter.com/theostoican"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">theostoican</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&quot;In God we trust. All others bring data!&quot;</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
